{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn import metrics\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def load_faq_csv(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    return data\n",
    "\n",
    "# Create a model of word2vec using the gensim library\n",
    "def load_word2vec(path):\n",
    "    mod = word2vec.KeyedVectors.load_word2vec_format(path, binary=True,)\n",
    "    return mod\n",
    "\n",
    "# Now we converting the vector of word(Number) to its actual word\n",
    "def convertVector_to_word(model):\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    return index2word_set\n",
    "\n",
    "# Remove stop words like \"I,this,that,these,and,the etc.\"\n",
    "def remove_stopwords(words):\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # return \" \".join(w for w in words if w not in stop_words)\n",
    "    return [w for w in words if w not in stop_words]\n",
    "\n",
    "def lowercase(words):\n",
    "    # return \" \".join(w.lower() for w in words)\n",
    "    return [w.lower() for w in words]\n",
    "\n",
    "# stemming of word\n",
    "def stem(words):\n",
    "    ps = PorterStemmer()\n",
    "    # return \" \".join(ps.stem(w) for w in words)\n",
    "    return [ps.stem(w) for w in words]\n",
    "\n",
    "def clean_text(sentence):\n",
    "    words = str(sentence).split()\n",
    "    lower_case_words = lowercase(words)\n",
    "    stop_words_removed =remove_stopwords(lower_case_words)\n",
    "    # stemmed_words=app.stem(stop_words_removed)\n",
    "    return stop_words_removed\n",
    "\n",
    "# here we will define the calculation of average word2vec for a sentence\n",
    "def average_word2vec_sentence(sentences, model, num_features, index2word_set):\n",
    "    l1=[]\n",
    "    for sent in sentences:\n",
    "        cleaned_words = clean_text(sent)\n",
    "        feature_vector = np.zeros((num_features,), dtype='float32')\n",
    "        n_words = 0\n",
    "        for word in cleaned_words:\n",
    "            if word in index2word_set:\n",
    "                n_words += 1\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "            if (n_words > 0):\n",
    "                feature_vector = np.divide(feature_vector, n_words)\n",
    "\n",
    "        l1.append(feature_vector)\n",
    "    return l1\n",
    "# here we will define the calculation of average word2vec for a sentence\n",
    "def average_word2vec_sentence1(sentence, model, num_features, index2word_set):\n",
    "    cleaned_words = clean_text(sentence)\n",
    "    feature_vector = np.zeros((num_features,), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in cleaned_words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "        if (n_words > 0):\n",
    "            feature_vector = np.divide(feature_vector, n_words)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "#split the data into train and test\n",
    "def split_data(data):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(data['Query'],data['Label'],random_state=42,test_size=0.5,shuffle=True)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "def Logistic_Regression(X_train_tvf ,X_test_tvf,y_train,y_test):\n",
    "    estimator=LogisticRegression()\n",
    "    param_grid={'C':[0.01,0.05,0.1,0.5,1,5,10],'penalty':['l1','l2']}\n",
    "    optimizer=GridSearchCV(estimator,param_grid,cv=10)\n",
    "    optimizer.fit(X_train_tvf,y_train)\n",
    "    predict=optimizer.best_estimator_.predict(X_test_tvf)\n",
    "    return metrics.accuracy_score(y_test,predict),optimizer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
